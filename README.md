# ScienceAgentBench-annotation

This repository showcases my annotation and evaluation work for the **ScienceAgentBench (SAB)** and **AutoSDT** projects â€” two large-scale efforts to build transparent and standardized scientific reasoning benchmarks for generalist AI agents.

---

## ðŸš€ My Contribution

- **Designed and implemented evaluation scripts** (`eval_programs/`) that verify agent outputs across diverse scientific domains, including survival analysis, geoscience, and biomedical modeling.  
  Each script defines reproducible evaluation metrics and data integrity checks for automatic benchmarking.

- **Developed and validated gold-standard programs** (`gold_programs/`) serving as the reference logic for SAB task correctness.  
  I ensured consistency between AutoSDT-generated code and benchmark expectations by refining program structure, handling exceptions, and improving readability.

- **Standardized data pipelines between AutoSDT and SAB**, enabling smooth execution of generated scripts within the evaluation framework.  
  This included preprocessing automation, directory structuring, and cross-system compatibility.

- **Led large-scale verification of benchmark tasks**, testing correctness across multiple datasets (e.g., METABRIC, MouseAtlas, Pancreas) and ensuring alignment between model outputs and gold results.

- **Optimized reproducibility and clarity**, restructuring the repository into lightweight, public-safe format (with heavy data moved to external storage).

---

## ðŸ§© Technical Focus

- Python (PyTorch, NumPy, Pandas, SciPy, Scanpy)  
- Automated evaluation scripting and task validation  
- Benchmark reproducibility and anti-contamination checks  
- Large-scale dataset handling under Git/GitHub constraints  
- Integration of AutoSDT-generated scientific task code with SAB evaluation layer  

---

## ðŸ“‚ Repository Overview

ScienceAgentBench-annotation/
â”‚
â”œâ”€â”€ eval_programs/ # My implemented evaluation scripts for benchmark tasks
â”œâ”€â”€ gold_programs/ # Verified gold-standard task logic
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md


---

## ðŸ”— External Resources

Due to GitHubâ€™s file size limits, large `.h5ad` and dataset files are hosted externally.  
Full benchmark data and outputs are available here:  
 [Google Drive: SAB-application](https://drive.google.com/drive/u/0/folders/1EJxOHzQV4Oeh8n2Q-2BbdR4P31S36Mai)


*This repository documents my hands-on contribution to the ScienceAgentBench benchmark and AutoSDT auto-annotation framework, focusing on reproducibility, correctness, and scientific AI evaluation.*
